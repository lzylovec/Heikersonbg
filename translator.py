import speech_recognition as sr
import dashscope
from dashscope import Generation
import os
from dotenv import load_dotenv
import json
import requests
from openai import OpenAI
import queue
import threading
from dashscope.audio.asr import Recognition
from http import HTTPStatus
import io
import wave
import pyaudio

load_dotenv()
api_key = os.getenv('DASHSCOPE_API_KEY')
# é…ç½®OpenAIå…¼å®¹æ¨¡å¼
client = OpenAI(
    api_key=api_key,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

dashscope.api_key = api_key

class SocialAnxietyTranslator:
    def __init__(self):
        self.recognizer = sr.Recognizer()
        
        # å°è¯•å¤šä¸ªéº¦å…‹é£è®¾å¤‡
        self.microphone = None
        self.stream_queue = queue.Queue()
        self.analysis_queue = queue.Queue()
        self.summary_queue = queue.Queue()
        self.segments_log = []
        self.analysis_log = []
        self._stop_listening = None
        self._streaming = False
        self._pa = None
        self._manual_stream = None
        self._manual_frames = []
        self._manual_recording = False
        self._manual_rate = 16000
        self._manual_channels = 1
        self._manual_chunk = 1024
        available_mics = sr.Microphone.list_microphone_names()
        print(f"å¯ç”¨éº¦å…‹é£è®¾å¤‡: {available_mics}")
        
        # å°è¯•æ‰¾åˆ°åˆé€‚çš„éº¦å…‹é£
        for i, mic_name in enumerate(available_mics):
            try:
                print(f"å°è¯•éº¦å…‹é£è®¾å¤‡ {i}: {mic_name}")
                test_mic = sr.Microphone(device_index=i, sample_rate=16000, chunk_size=1024)
                with test_mic as source:
                    self.recognizer.adjust_for_ambient_noise(source, duration=0.5)
                self.microphone = test_mic
                print(f"âœ… ä½¿ç”¨éº¦å…‹é£è®¾å¤‡ {i}: {mic_name}")
                break
            except Exception as e:
                print(f"âŒ è®¾å¤‡ {i} ä¸å¯ç”¨: {e}")
                continue
        
        if self.microphone is None:
            # ä½¿ç”¨é»˜è®¤éº¦å…‹é£
            try:
                self.microphone = sr.Microphone(sample_rate=16000, chunk_size=1024)
                print("ä½¿ç”¨é»˜è®¤éº¦å…‹é£")
            except Exception as e:
                print(f"âŒ æ— æ³•åˆå§‹åŒ–éº¦å…‹é£: {e}")
                raise Exception("æ— æ³•æ‰¾åˆ°å¯ç”¨çš„éº¦å…‹é£è®¾å¤‡")

    

    def start_streaming(self):
        if self._streaming:
            return True
        self._reset_stream_state()
        def _callback(recognizer, audio):
            try:
                text = recognizer.recognize_google(audio, language='zh-CN')
                if text:
                    self.segments_log.append(text)
                    self.stream_queue.put(text)
                    threading.Thread(target=self._analyze_segment, args=(text,), daemon=True).start()
            except Exception:
                pass
        try:
            self._stop_listening = self.recognizer.listen_in_background(self.microphone, _callback, phrase_time_limit=5)
            self._streaming = True
            return True
        except Exception as e:
            print(f"âŒ æ— æ³•å¯åŠ¨è¿ç»­è½¬å†™: {e}")
            self._streaming = False
            return False

    def stop_streaming(self):
        if self._stop_listening:
            try:
                self._stop_listening(wait_for_stop=False)
            except Exception:
                pass
        self._streaming = False

    def start_manual_recording(self):
        if self._manual_recording:
            return True
        try:
            self._pa = pyaudio.PyAudio()
            _kwargs = {}
            try:
                _dev = getattr(self.microphone, 'device_index', None)
                if _dev is not None:
                    _kwargs['input_device_index'] = _dev
            except Exception:
                pass
            self._manual_stream = self._pa.open(format=pyaudio.paInt16, channels=self._manual_channels, rate=self._manual_rate, input=True, frames_per_buffer=self._manual_chunk, **_kwargs)
            self._manual_frames = []
            self._manual_recording = True
            def _capture():
                while self._manual_recording:
                    try:
                        data = self._manual_stream.read(self._manual_chunk, exception_on_overflow=False)
                        self._manual_frames.append(data)
                    except Exception:
                        break
            threading.Thread(target=_capture, daemon=True).start()
            return True
        except Exception as e:
            print(f"âŒ æ— æ³•å¼€å§‹æ‰‹åŠ¨å½•éŸ³: {e}")
            self._manual_recording = False
            return False

    def stop_manual_recording(self):
        if not self._manual_recording and not self._manual_frames:
            return None
        try:
            self._manual_recording = False
            try:
                if self._manual_stream:
                    self._manual_stream.stop_stream()
                    self._manual_stream.close()
            except Exception:
                pass
            try:
                if self._pa:
                    self._pa.terminate()
            except Exception:
                pass
            pcm = b''.join(self._manual_frames)
            self._manual_frames = []
            buf = io.BytesIO()
            wf = wave.open(buf, 'wb')
            wf.setnchannels(self._manual_channels)
            wf.setsampwidth(2)
            wf.setframerate(self._manual_rate)
            wf.writeframes(pcm)
            wf.close()
            buf.seek(0)
            with sr.AudioFile(buf) as source:
                audio = self.recognizer.record(source)
            try:
                text = self.recognizer.recognize_google(audio, language='zh-CN')
                return text
            except Exception:
                try:
                    text = self.recognizer.recognize_sphinx(audio, language='zh-CN')
                    return text
                except Exception:
                    pass
            return None
        except Exception as e:
            print(f"âŒ æ‰‹åŠ¨å½•éŸ³å¤„ç†å¤±è´¥: {e}")
            return None

    def _reset_stream_state(self):
        try:
            while True:
                self.stream_queue.get_nowait()
        except queue.Empty:
            pass
        try:
            while True:
                self.analysis_queue.get_nowait()
        except queue.Empty:
            pass
        try:
            while True:
                self.summary_queue.get_nowait()
        except queue.Empty:
            pass
        self.segments_log.clear()
        self.analysis_log.clear()

    def _analyze_segment(self, text):
        try:
            print(f"åˆ†æåˆ†å¥: {text}")
            completion = client.chat.completions.create(
                model="qwen3-max",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç¤¾äº¤æ„å›¾åˆ†æä¸“å®¶ï¼Œè¯†åˆ«ä¸­æ–‡å®¢å¥—è¯å¹¶ç»™å‡ºçœŸå®æ„å›¾ä¸å»ºè®®å›åº”"},
                    {"role": "user", "content": f"æ–‡æœ¬ï¼š{text}\nè¯·è¾“å‡ºï¼šç±»å‹ã€çœŸå®æ„å›¾ã€å»ºè®®å›åº”"}
                ],
                stream=False,
                temperature=0.2
            )
            result = completion.choices[0].message.content
            self.analysis_log.append(result)
            self.analysis_queue.put(result)
            threading.Thread(target=self._update_summary, daemon=True).start()
        except Exception as e:
            print(f"AIåˆ†æå¤±è´¥: {e}")
            try:
                response = Generation.call(
                    model='qwen-turbo',
                    prompt=f"è¯·åˆ†ææ˜¯å¦ä¸ºå®¢å¥—è¯ï¼Œå¹¶ç»™å‡ºçœŸå®æ„å›¾ä¸å»ºè®®å›åº”ï¼š{text}",
                    stream=False,
                    temperature=0.2
                )
                if hasattr(response, 'status_code') and response.status_code == 200:
                    self.analysis_log.append(response.output.text)
                    self.analysis_queue.put(response.output.text)
                    threading.Thread(target=self._update_summary, daemon=True).start()
                else:
                    self.analysis_queue.put(f"åˆ†æå¤±è´¥: {getattr(response, 'message', 'unknown error')}")
            except Exception as e2:
                self.analysis_queue.put(f"åˆ†æå¤±è´¥: {e2}")

    def _update_summary(self):
        try:
            last_segments = self.segments_log[-5:]
            last_analyses = self.analysis_log[-5:]
            content = "\n".join([f"- è¯­å¥: {s}" for s in last_segments]) + "\n" + \
                      "\n".join([f"- åˆ†æ: {a}" for a in last_analyses])
            completion = client.chat.completions.create(
                model="qwen3-max",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯æ‘˜è¦åŠ©æ‰‹ï¼Œè¯·å°†æœ€è¿‘è¯­å¥ä¸åˆ†ææ€»ç»“ä¸ºç®€æ´ä¸­æ–‡è¦ç‚¹ï¼Œçªå‡ºçœŸå®æ„å›¾ä¸äº’åŠ¨å»ºè®®"},
                    {"role": "user", "content": f"è¯·åŸºäºä»¥ä¸‹å†…å®¹ç”Ÿæˆä¸è¶…è¿‡5æ¡çš„è¦ç‚¹æ‘˜è¦ï¼š\n{content}"}
                ],
                stream=False,
                temperature=0.2
            )
            summary = completion.choices[0].message.content
            self.summary_queue.put(summary)
        except Exception as e:
            try:
                response = Generation.call(
                    model='qwen-turbo',
                    prompt=f"è¯·å¯¹ä»¥ä¸‹å†…å®¹ç”Ÿæˆç®€æ´è¦ç‚¹æ‘˜è¦ï¼š\n{content}",
                    stream=False,
                    temperature=0.2
                )
                if hasattr(response, 'status_code') and response.status_code == 200:
                    self.summary_queue.put(response.output.text)
                else:
                    self.summary_queue.put(f"æ‘˜è¦å¤±è´¥: {getattr(response, 'message', 'unknown error')}")
            except Exception as e2:
                self.summary_queue.put(f"æ‘˜è¦å¤±è´¥: {e2}")
        
    def speech_to_text(self):
        """å°†è¯­éŸ³è½¬æ¢ä¸ºæ–‡æœ¬"""
        if self.microphone is None:
            print("âŒ éº¦å…‹é£æœªåˆå§‹åŒ–")
            return None
            
        try:
            print("ğŸ¤ æ­£åœ¨æ¿€æ´»éº¦å…‹é£...")
            
            # é‡æ–°æ£€æŸ¥éº¦å…‹é£çŠ¶æ€
            with self.microphone as source:
                print("âœ… éº¦å…‹é£å·²æ¿€æ´»")
                print("ğŸ¤ è¯·è¯´è¯...ï¼ˆæœ€å¤š10ç§’ï¼‰")
                
                # è°ƒæ•´ç¯å¢ƒå™ªéŸ³
                print("ğŸ”Š è°ƒæ•´ç¯å¢ƒå™ªéŸ³...")
                self.recognizer.adjust_for_ambient_noise(source, duration=1)
                print("âœ… ç¯å¢ƒå™ªéŸ³è°ƒæ•´å®Œæˆ")
                
                # ç›‘å¬è¯­éŸ³ - å…³é”®æ­¥éª¤ï¼
                print("ğŸ‘‚ æ­£åœ¨ç›‘å¬è¯­éŸ³ï¼Œè¯·è¯´è¯...")
                print("ğŸ’¡ æç¤ºï¼šå½“éº¦å…‹é£çœŸæ­£å¯ç”¨æ—¶ï¼Œè¿™é‡Œä¼šæ˜¾ç¤ºå½•éŸ³çŠ¶æ€")
                
                # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´å’Œæ›´æ•æ„Ÿçš„æ£€æµ‹
                audio = self.recognizer.listen(
                    source, 
                    timeout=15,  # æ›´é•¿çš„è¶…æ—¶æ—¶é—´
                    phrase_time_limit=10  # çŸ­è¯­æ—¶é—´é™åˆ¶
                )
                print("âœ… å½•éŸ³å®Œæˆï¼æ£€æµ‹åˆ°è¯­éŸ³è¾“å…¥")
                
        except sr.WaitTimeoutError:
            print("âŒ ç­‰å¾…è¶…æ—¶ï¼Œæœªæ£€æµ‹åˆ°è¯­éŸ³è¾“å…¥")
            print("ğŸ’¡ å¯èƒ½åŸå› ï¼š")
            print("   - éº¦å…‹é£æƒé™æœªå¼€å¯")
            print("   - éº¦å…‹é£ç¡¬ä»¶é—®é¢˜")
            print("   - ç¯å¢ƒå¤ªå®‰é™æˆ–å£°éŸ³å¤ªå°")
            return None
        except Exception as e:
            print(f"âŒ å½•éŸ³è¿‡ç¨‹å‡ºé”™: {e}")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            return None
            
        try:
            print("ğŸ§  æ­£åœ¨è¯†åˆ«è¯­éŸ³...")
            text = self.recognizer.recognize_google(audio, language='zh-CN')
            print(f"ğŸ¯ è¯†åˆ«åˆ°çš„æ–‡æœ¬: {text}")
            return text
        except sr.UnknownValueError:
            print("âŒ æ— æ³•è¯†åˆ«è¯­éŸ³å†…å®¹")
            print("ğŸ”„ å°è¯•å¤‡ç”¨è¯†åˆ«å¼•æ“...")
            try:
                text = self.recognizer.recognize_sphinx(audio, language='zh-CN')
                print(f"ğŸ¯ Sphinxè¯†åˆ«ç»“æœ: {text}")
                return text
            except Exception as sphinx_e:
                print(f"âŒ å¤‡ç”¨è¯†åˆ«ä¹Ÿå¤±è´¥: {sphinx_e}")
                return None
        except sr.RequestError as e:
            print(f"âŒ è¯­éŸ³è¯†åˆ«æœåŠ¡é”™è¯¯: {e}")
            print("ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            return None
        
    def translate_politeness(self, text):
        """ä½¿ç”¨å¤§æ¨¡å‹åˆ¤æ–­æ˜¯å¦ä¸ºå®¢å¥—è¯å¹¶ç¿»è¯‘çœŸå®æ„å›¾"""
        if not text:
            return None
            
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªç¤¾äº¤æ„å›¾åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬ï¼Œåˆ¤æ–­è¯´è¯è€…æ˜¯å¦åœ¨è¯´å®¢å¥—è¯ï¼Œ
        å¹¶ç»™å‡ºå…¶çœŸå®æ„å›¾ã€‚å¦‚æœæ˜¯å®¢å¥—è¯ï¼Œè¯·ç›´æ¥ç¿»è¯‘å‡ºçœŸå®å«ä¹‰ï¼›å¦‚æœä¸æ˜¯å®¢å¥—è¯ï¼Œ
        è¯·è¯´æ˜è¿™æ˜¯çœŸè¯šçš„è¡¨è¾¾ã€‚
        
        æ–‡æœ¬: "{text}"
        
        è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›å¤ï¼š
        åˆ†æ: [ç®€è¦åˆ†æ]
        ç±»å‹: [å®¢å¥—è¯/çœŸè¯šè¡¨è¾¾]
        çœŸå®æ„å›¾: [ç¿»è¯‘åçš„çœŸå®å«ä¹‰ï¼Œå¦‚æœæ˜¯å®¢å¥—è¯]
        å»ºè®®å›åº”: [ç»™ç¤¾æäººå£«çš„å»ºè®®å›åº”]
        """
        
        try:
            # ä½¿ç”¨OpenAIå…¼å®¹æ¨¡å¼è°ƒç”¨qwen3-max
            print(f"ğŸ¤– æ­£åœ¨è°ƒç”¨qwen3-maxæ¨¡å‹åˆ†ææ–‡æœ¬: {text}")
            
            completion = client.chat.completions.create(
                model="qwen3-max",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¤¾äº¤æ„å›¾åˆ†æä¸“å®¶ï¼Œæ“…é•¿è¯†åˆ«ä¸­æ–‡å®¢å¥—è¯å’Œåˆ†æçœŸå®æ„å›¾ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                stream=False,
                temperature=0.7
            )
            
            result = completion.choices[0].message.content
            print(f"âœ… AIåˆ†æç»“æœ: {result}")
            return result
            
        except Exception as e:
            print(f"âŒ è°ƒç”¨å¤§æ¨¡å‹APIæ—¶å‡ºé”™: {e}")
            print("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•...")
            
            # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨åŸæ¥çš„dashscopeæ–¹æ³•
            try:
                response = Generation.call(
                    model='qwen-turbo',
                    prompt=prompt,
                    stream=False,
                    temperature=0.7
                )
                
                if response.status_code == 200:
                    result = response.output.text
                    print(f"âœ… å¤‡ç”¨æ–¹æ³•æˆåŠŸ: {result}")
                    return result
                else:
                    print(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {response.status_code}")
                    return None
                    
            except Exception as e2:
                print(f"âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥: {e2}")
                return None
        
    def process_audio(self):
        """å®Œæ•´çš„è¯­éŸ³å¤„ç†æµç¨‹"""
        print("=== ç¤¾æç¿»è¯‘å™¨å¯åŠ¨ ===")
        
        # è¯­éŸ³è¯†åˆ«
        text = self.speech_to_text()
        if not text:
            return None
        
        # AIç¿»è¯‘
        translation = self.translate_politeness(text)
        return {
            'original_text': text,
            'translation': translation
        }

    def speech_to_text_with_progress(self, on_status, max_attempts=3):
        if self.microphone is None:
            return None
        for attempt in range(1, max_attempts + 1):
            try:
                on_status('ğŸ¤ æ­£åœ¨æ¿€æ´»éº¦å…‹é£...')
                with self.microphone as source:
                    on_status('âœ… éº¦å…‹é£å·²æ¿€æ´»')
                    on_status('ğŸ”Š æ­£åœ¨æ ¡å‡†ç¯å¢ƒå™ªéŸ³...')
                    self.recognizer.adjust_for_ambient_noise(source, duration=0.8)
                    self.recognizer.dynamic_energy_threshold = True
                    self.recognizer.pause_threshold = 0.5
                    self.recognizer.non_speaking_duration = 0.15
                    on_status(f'ğŸ¤ éº¦å…‹é£å°±ç»ªï¼ˆç¬¬{attempt}æ¬¡ï¼‰ï¼Œè¯·å¼€å§‹è¯´è¯')
                    audio = self.recognizer.listen(
                        source,
                        timeout=15,
                        phrase_time_limit=8
                    )
                    on_status('â¹ï¸ å½•éŸ³å®Œæˆï¼Œæ­£åœ¨è¯†åˆ«...')
            except sr.WaitTimeoutError:
                on_status(f'âŒ æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼ˆç¬¬{attempt}æ¬¡ï¼‰ï¼Œæ­£åœ¨é‡è¯•')
                continue
            except Exception:
                on_status(f'âŒ å½•éŸ³å‡ºé”™ï¼ˆç¬¬{attempt}æ¬¡ï¼‰ï¼Œæ­£åœ¨é‡è¯•')
                continue
            # è¯†åˆ«é˜¶æ®µ
            try:
                on_status('ğŸ§  æ­£åœ¨è¯†åˆ«è¯­éŸ³...')
                result = self.recognizer.recognize_google(audio, language='zh-CN', show_all=True)
                if isinstance(result, dict) and 'alternative' in result and len(result['alternative']) > 0:
                    text = result['alternative'][0].get('transcript', '')
                else:
                    text = result if isinstance(result, str) else ''
                if text and text.strip():
                    on_status('âœ… å·²è¯†åˆ«ï¼Œæ­£åœ¨åˆ†æ...')
                    return text.strip()
                else:
                    raise sr.UnknownValueError()
            except sr.UnknownValueError:
                on_status(f'âŒ æ— æ³•è¯†åˆ«ï¼ˆç¬¬{attempt}æ¬¡ï¼‰ï¼Œå°è¯•å¤‡ç”¨è¯†åˆ«')
                try:
                    text = self.recognizer.recognize_sphinx(audio, language='zh-CN')
                    if text and text.strip():
                        on_status('âœ… å·²è¯†åˆ«ï¼Œæ­£åœ¨åˆ†æ...')
                        return text.strip()
                except Exception:
                    pass
                try:
                    wav_path = "/tmp/social_anxiety_input.wav"
                    wav_data = audio.get_wav_data(convert_rate=16000, convert_width=2)
                    with open(wav_path, "wb") as f:
                        f.write(wav_data)
                    recognition = Recognition(model='paraformer-realtime-v2', format='wav', sample_rate=16000, language_hints=['zh'], callback=None)
                    result = recognition.call(wav_path)
                    if result.status_code == HTTPStatus.OK:
                        sentences = result.get_sentence()
                        joined = "".join([s.get('text') if isinstance(s, dict) else str(s) for s in sentences])
                        if joined.strip():
                            on_status('âœ… å·²è¯†åˆ«ï¼Œæ­£åœ¨åˆ†æ...')
                            return joined.strip()
                except Exception:
                    pass
                on_status(f'âŒ è¯†åˆ«å¤±è´¥ï¼ˆç¬¬{attempt}æ¬¡ï¼‰')
                continue
            except sr.RequestError:
                on_status(f'âŒ è¯­éŸ³è¯†åˆ«æœåŠ¡é”™è¯¯ï¼ˆç¬¬{attempt}æ¬¡ï¼‰')
                continue
        on_status('âŒ å¤šæ¬¡å°è¯•ä»æœªè¯†åˆ«ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£å¹¶é‡è¯•')
        return None

if __name__ == "__main__":
    translator = SocialAnxietyTranslator()
    result = translator.process_audio()
    
    if result:
        print("\n=== æœ€ç»ˆç»“æœ ===")
        print(f"åŸè¯: {result['original_text']}")
        print(f"ç¿»è¯‘: {result['translation']}")
    else:
        print("å¤„ç†å¤±è´¥")